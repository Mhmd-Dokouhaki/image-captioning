{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning - Feature Extraction\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll extract meaningful visual features from our images using pre-trained Convolutional Neural Networks (CNNs). This is a crucial step in our image captioning pipeline for several reasons:\n",
    "\n",
    "1. **Transfer Learning**: Rather than training a vision model from scratch (which would require massive computational resources and data), we leverage pre-trained models that have already learned rich visual representations from millions of images.\n",
    "\n",
    "2. **Computational Efficiency**: By pre-computing image features just once and saving them to disk, we can significantly speed up the training of our captioning model, as we won't need to pass images through the CNN during each training epoch.\n",
    "\n",
    "3. **Architectural Decoupling**: Separating feature extraction from caption generation allows us to experiment with different language models without re-processing the images.\n",
    "\n",
    "We'll extract features using several different CNN architectures and compare their characteristics in terms of:\n",
    "- Feature dimensionality\n",
    "- Extraction speed\n",
    "- Memory requirements\n",
    "- Feature distribution properties\n",
    "\n",
    "This comparison will help us make an informed decision about which architecture to use for our image captioning system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project root to path to allow importing from other modules\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from models.encoder import EncoderCNN\n",
    "from utils.vocabulary import Vocabulary\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Transfer Learning for Image Feature Extraction\n",
    "\n",
    "Before diving into the code, let's understand the concept of transfer learning and why it's so valuable for our task.\n",
    "\n",
    "### What is Transfer Learning?\n",
    "\n",
    "Transfer learning is a machine learning technique where a model developed for one task is reused as the starting point for a model on a second task. For computer vision, this typically means using CNNs pre-trained on large image datasets like ImageNet.\n",
    "\n",
    "### Why Use Pre-trained Models?\n",
    "\n",
    "1. **Knowledge Transfer**: Early layers of CNNs learn general features (edges, textures) while deeper layers learn more task-specific features. The general features are useful across many vision tasks.\n",
    "\n",
    "2. **Efficiency**: Training a CNN from scratch requires massive computational resources and large labeled datasets.\n",
    "\n",
    "3. **Performance**: Models initialized with pre-trained weights often achieve better performance than those trained from scratch, especially with limited data.\n",
    "\n",
    "### How We'll Use Transfer Learning\n",
    "\n",
    "For our image captioning task, we'll:\n",
    "\n",
    "1. Take a CNN pre-trained on ImageNet (e.g., ResNet, MobileNet or InceptionV3)\n",
    "2. Remove the final classification layer\n",
    "3. Use the remaining network as a fixed feature extractor\n",
    "4. Extract and save features for all images in our dataset\n",
    "5. These features will become the input to our caption generator (RNN/LSTM)\n",
    "\n",
    "Let's visualize this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Create a visualization of the transfer learning process\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "# Create boxes for the visualization\n",
    "ax.add_patch(Rectangle((0, 0), 3, 4, facecolor='lightblue', alpha=0.5))\n",
    "ax.add_patch(Rectangle((3, 0), 7, 4, facecolor='lightgreen', alpha=0.5))\n",
    "ax.add_patch(Rectangle((10, 0), 3, 4, facecolor='salmon', alpha=0.5))\n",
    "\n",
    "# Add labels\n",
    "ax.text(1.5, 2, 'Input\\nImage', ha='center', va='center', fontsize=14)\n",
    "ax.text(6.5, 2, 'Pre-trained CNN\\n(Feature Extractor)', ha='center', va='center', fontsize=14)\n",
    "ax.text(11.5, 2, 'Image\\nFeatures', ha='center', va='center', fontsize=14)\n",
    "\n",
    "# Add arrows\n",
    "ax.arrow(3, 2, -0.5, 0, head_width=0.2, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(10, 2, -0.5, 0, head_width=0.2, head_length=0.1, fc='black', ec='black')\n",
    "ax.arrow(13, 2, 0.5, 0, head_width=0.2, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Text for removed classification layer\n",
    "ax.text(14, 2, 'Original classification\\nlayer removed', ha='left', va='center', fontsize=12)\n",
    "\n",
    "# Set axis limits and remove ticks\n",
    "ax.set_xlim(-0.5, 17)\n",
    "ax.set_ylim(-0.5, 4.5)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Feature Extraction using Pre-trained CNN', fontsize=16)\n",
    "\n",
    "# Remove axis borders\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a Dataset for Feature Extraction\n",
    "\n",
    "Now, let's implement a PyTorch `Dataset` class that will load and preprocess our images for feature extraction. This class will:\n",
    "\n",
    "1. Load images from disk\n",
    "2. Apply necessary transformations (resizing, normalization, etc.)\n",
    "3. Return the preprocessed images along with their filenames\n",
    "\n",
    "The transformations we apply are crucial and must match those used during training of the pre-trained models to ensure the extracted features are meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Dataset for loading images for feature extraction.\"\"\"\n",
    "    \n",
    "    def __init__(self, images_dir, image_list, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            images_dir (str): Directory containing the images\n",
    "            image_list (list): List of image filenames\n",
    "            transform (callable, optional): Transform to apply to the images\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.image_list = image_list\n",
    "        \n",
    "        # Define default transform if none is provided\n",
    "        # These transformations match those used during training of the pre-trained models\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                # Resize to 256x256 while maintaining aspect ratio\n",
    "                transforms.Resize(256),\n",
    "                # Center crop to 224x224 (standard input size for many CNNs)\n",
    "                transforms.CenterCrop(224),\n",
    "                # Convert PIL image to PyTorch tensor (0-1 range)\n",
    "                transforms.ToTensor(),\n",
    "                # Normalize using ImageNet mean and std\n",
    "                # This is crucial as the pre-trained models expect this normalization\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "                    std=[0.229, 0.224, 0.225]    # ImageNet std\n",
    "                )\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset\"\"\"\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get a sample from the dataset.\"\"\"\n",
    "        image = ...\n",
    "        img_name = ...\n",
    "        # TODO: Implement the method to get a sample from the dataset\n",
    "        # 1. Get the image filename at the given index\n",
    "        # 2. Construct the full path to the image \n",
    "        # 3. Load the image using PIL and convert to RGB format\n",
    "        # 4. Apply the transformations to the image\n",
    "        # 5. Return the transformed image and its filename\n",
    "        \n",
    "        return image, img_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the Image Transformations\n",
    "\n",
    "The transformations we apply in our dataset are critical for ensuring compatibility with pre-trained models:\n",
    "\n",
    "1. **Resize(256)**: First, we resize the image to have a smaller dimension of 256 pixels while maintaining the aspect ratio. This standardizes the image size while preserving the content.\n",
    "\n",
    "2. **CenterCrop(224)**: We then take a center crop of 224Ã—224 pixels. This is the standard input size for many CNN architectures trained on ImageNet. The center crop helps focus on the main subject of the image.\n",
    "\n",
    "3. **ToTensor()**: This converts the PIL image (with pixel values 0-255) to a PyTorch tensor with values normalized to the range [0.0, 1.0].\n",
    "\n",
    "4. **Normalize()**: Finally, we normalize each color channel using the mean and standard deviation of the ImageNet dataset. This ensures the input distribution matches what the pre-trained network expects.\n",
    "\n",
    "Let's look at how these transformations affect an example image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directory and paths\n",
    "data_dir = '../data/flickr8k/processed'\n",
    "images_dir = os.path.join(data_dir, 'images')\n",
    "features_dir = os.path.join(data_dir, 'features')\n",
    "os.makedirs(features_dir, exist_ok=True)\n",
    "\n",
    "# Load captions file to get list of image names\n",
    "captions_path = os.path.join(data_dir, 'captions.csv')\n",
    "if os.path.exists(captions_path):\n",
    "    captions_df = pd.read_csv(captions_path)\n",
    "    # Get unique image names\n",
    "    image_names = captions_df['image'].unique().tolist()\n",
    "    print(f\"Found {len(image_names)} unique images in the captions file\")\n",
    "    \n",
    "    # Display a sample image with transformations\n",
    "    if len(image_names) > 0:\n",
    "        # Select a random image\n",
    "        import random\n",
    "        sample_img_name = random.choice(image_names)\n",
    "        img_path = os.path.join(images_dir, sample_img_name)\n",
    "        \n",
    "        if os.path.exists(img_path):\n",
    "            # Load the original image\n",
    "            original_img = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            # Create individual transformations for visualization\n",
    "            resize_transform = transforms.Resize(256)\n",
    "            crop_transform = transforms.CenterCrop(224)\n",
    "            tensor_transform = transforms.ToTensor()\n",
    "            normalize_transform = transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "            \n",
    "            # Apply transformations step by step\n",
    "            resized_img = resize_transform(original_img)\n",
    "            cropped_img = crop_transform(resized_img)\n",
    "            tensor_img = tensor_transform(cropped_img)\n",
    "            normalized_img = normalize_transform(tensor_img)\n",
    "            \n",
    "            # Convert tensors back to PIL images for display\n",
    "            tensor_img_display = tensor_img.permute(1, 2, 0).numpy()\n",
    "            \n",
    "            # For normalized image, we need to denormalize for display\n",
    "            normalized_img_display = normalized_img.permute(1, 2, 0).numpy()\n",
    "            normalized_img_display = normalized_img_display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "            normalized_img_display = np.clip(normalized_img_display, 0, 1)\n",
    "            \n",
    "            # Display the transformation steps\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "            \n",
    "            axes[0].imshow(original_img)\n",
    "            axes[0].set_title(f'Original ({original_img.size[0]}x{original_img.size[1]})')\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            axes[1].imshow(resized_img)\n",
    "            axes[1].set_title(f'Resized ({resized_img.size[0]}x{resized_img.size[1]})')\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            axes[2].imshow(cropped_img)\n",
    "            axes[2].set_title(f'Cropped ({cropped_img.size[0]}x{cropped_img.size[1]})')\n",
    "            axes[2].axis('off')\n",
    "            \n",
    "            axes[3].imshow(normalized_img_display)\n",
    "            axes[3].set_title('Normalized')\n",
    "            axes[3].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Show pixel value distributions before and after normalization\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            # Before normalization (values between 0 and 1)\n",
    "            for i, color in enumerate(['red', 'green', 'blue']):\n",
    "                sns.kdeplot(tensor_img[i].numpy().flatten(), ax=axes[0], label=color, color=color)\n",
    "            axes[0].set_title('Pixel Distribution Before Normalization')\n",
    "            axes[0].set_xlabel('Pixel Value (0-1)')\n",
    "            axes[0].set_ylabel('Density')\n",
    "            axes[0].legend()\n",
    "            \n",
    "            # After normalization (values generally between -2 and 2)\n",
    "            for i, color in enumerate(['red', 'green', 'blue']):\n",
    "                sns.kdeplot(normalized_img[i].numpy().flatten(), ax=axes[1], label=color, color=color)\n",
    "            axes[1].set_title('Pixel Distribution After Normalization')\n",
    "            axes[1].set_xlabel('Normalized Value')\n",
    "            axes[1].set_ylabel('Density')\n",
    "            axes[1].legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Sample image not found at path: {img_path}\")\n",
    "else:\n",
    "    print(f\"Captions file not found at path: {captions_path}\")\n",
    "    # Create a small sample set for demonstration\n",
    "    image_names = [f\"sample_image_{i}.jpg\" for i in range(10)]\n",
    "    print(f\"Created {len(image_names)} sample image names for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction Function\n",
    "\n",
    "Now, we'll define a function to extract features from images using a pre-trained CNN model. This function will:\n",
    "\n",
    "1. Process images in batches through the model\n",
    "2. Extract features from the penultimate layer\n",
    "3. Store the features in an HDF5 file for efficient storage and retrieval\n",
    "4. Track and report time performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dataloader, model, output_file, device='cuda'):\n",
    "    \"\"\"\n",
    "    Extract features from images using a CNN model.\n",
    "    \n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader for the images\n",
    "        model (nn.Module): CNN model for feature extraction\n",
    "        output_file (str): Path to save the features\n",
    "        device (str): Device to use ('cuda' or 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of extracted features\n",
    "    \"\"\"\n",
    "    features = ...\n",
    "    # TODO: Implement feature extraction\n",
    "    # 1. Move model to device and set to evaluation mode\n",
    "    # 2. Create a dictionary to store features\n",
    "    # 3. Track timing for performance analysis\n",
    "    # 4. Process images in batches using torch.no_grad()\n",
    "    # 5. Extract features using the model\n",
    "    # 6. Store features with image names as keys\n",
    "    # 7. Calculate and print timing statistics\n",
    "    # 8. Save features to HDF5 file\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Dataset and DataLoader\n",
    "\n",
    "Now, let's create our dataset and dataloader for feature extraction. The dataloader will efficiently batch the images for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "image_dataset = ImageDataset(images_dir, image_names)\n",
    "\n",
    "# Create data loader with batching and parallel loading\n",
    "dataloader = DataLoader(\n",
    "    image_dataset,\n",
    "    batch_size=32,               # Process 32 images at once\n",
    "    shuffle=False,               # No need to shuffle for feature extraction\n",
    "    num_workers=4,               # Use 4 worker threads for data loading\n",
    "    pin_memory=True              # Pin tensors in memory for faster GPU transfer\n",
    ")\n",
    "\n",
    "print(f\"Created dataloader with {len(dataloader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define CNN Models for Feature Extraction\n",
    "\n",
    "We'll now define several CNN architectures to use for feature extraction. We'll compare:\n",
    "\n",
    "1. **ResNet18**: A relatively lightweight model (11M parameters) with 512-dimensional features\n",
    "2. **ResNet50**: A deeper model (25M parameters) with 2048-dimensional features\n",
    "3. **MobileNetV2**: A mobile-optimized model (3.5M parameters) with 1280-dimensional features\n",
    "\n",
    "Each model offers different trade-offs between computational efficiency and feature quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN models to use for feature extraction\n",
    "cnn_models = {\n",
    "    'resnet18': {\n",
    "        'model': EncoderCNN(model_name='resnet18', embed_size=256),\n",
    "        'output_file': os.path.join(features_dir, 'resnet18_features.h5'),\n",
    "        'feature_size': 512,\n",
    "        'params': 11.7,  # Millions of parameters\n",
    "        'description': 'Lightweight model with good performance/speed tradeoff'\n",
    "    },\n",
    "    'resnet50': {\n",
    "        'model': EncoderCNN(model_name='resnet50', embed_size=256),\n",
    "        'output_file': os.path.join(features_dir, 'resnet50_features.h5'),\n",
    "        'feature_size': 2048,\n",
    "        'params': 25.6,\n",
    "        'description': 'Deeper model with higher-dimensional features'\n",
    "    },\n",
    "    'mobilenet_v2': {\n",
    "        'model': EncoderCNN(model_name='mobilenet_v2', embed_size=256),\n",
    "        'output_file': os.path.join(features_dir, 'mobilenet_v2_features.h5'),\n",
    "        'feature_size': 1280,\n",
    "        'params': 3.5,\n",
    "        'description': 'Efficient model designed for mobile devices'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display model information in a table\n",
    "model_info = []\n",
    "for name, config in cnn_models.items():\n",
    "    model_info.append({\n",
    "        'Model': name,\n",
    "        'Feature Dim': config['feature_size'],\n",
    "        'Parameters (M)': config['params'],\n",
    "        'Description': config['description']\n",
    "    })\n",
    "\n",
    "pd.DataFrame(model_info).set_index('Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract Features with Each Model\n",
    "\n",
    "Now we'll extract features using each of our defined CNN models. This process might take some time, especially for larger models and datasets.\n",
    "\n",
    "If you're running on a system with limited GPU memory, you might want to comment out the larger models (ResNet50) and focus on the smaller ones first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for each model\n",
    "results = {}\n",
    "\n",
    "for name, config in cnn_models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Extracting features using {name}...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Skip if features already exist\n",
    "    if os.path.exists(config['output_file']):\n",
    "        print(f\"Features for {name} already exist at {config['output_file']}\")\n",
    "        print(f\"Size: {os.path.getsize(config['output_file']) / (1024 * 1024):.2f} MB\")\n",
    "        \n",
    "        # Load a sample of features to analyze\n",
    "        with h5py.File(config['output_file'], 'r') as h5f:\n",
    "            # Get a list of all dataset names (image filenames)\n",
    "            image_names = list(h5f.keys())\n",
    "            # Get the shape of the first dataset\n",
    "            feature_shape = h5f[image_names[0]].shape\n",
    "            \n",
    "            results[name] = {\n",
    "                'num_features': len(image_names),\n",
    "                'feature_shape': feature_shape,\n",
    "                'output_file': config['output_file']\n",
    "            }\n",
    "        \n",
    "        print(f\"Loaded info for {len(image_names)} features with shape {feature_shape}\")\n",
    "        continue\n",
    "    \n",
    "    # Extract features\n",
    "    features = extract_features(\n",
    "        dataloader=dataloader,\n",
    "        model=config['model'],\n",
    "        output_file=config['output_file'],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'num_features': len(features),\n",
    "        'feature_shape': next(iter(features.values())).shape,\n",
    "        'output_file': config['output_file']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Model Performance\n",
    "\n",
    "Let's summarize and compare the performance of each model in terms of feature dimensionality, storage requirements, and processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results summary\n",
    "summary = []\n",
    "for name, result in results.items():\n",
    "    file_size_mb = os.path.getsize(result['output_file']) / (1024 * 1024)\n",
    "    file_size_per_image_kb = file_size_mb * 1024 / result['num_features']\n",
    "    \n",
    "    summary.append({\n",
    "        'Model': name,\n",
    "        'Feature Dimension': result['feature_shape'],\n",
    "        'Number of Images': result['num_features'],\n",
    "        'Total Storage (MB)': file_size_mb,\n",
    "        'Storage per Image (KB)': file_size_per_image_kb\n",
    "    })\n",
    "\n",
    "# Create a summary dataframe\n",
    "summary_df = pd.DataFrame(summary).set_index('Model')\n",
    "\n",
    "# Format the numeric columns\n",
    "summary_df['Total Storage (MB)'] = summary_df['Total Storage (MB)'].map('{:.2f}'.format)\n",
    "summary_df['Storage per Image (KB)'] = summary_df['Storage per Image (KB)'].map('{:.2f}'.format)\n",
    "\n",
    "# Display the summary\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Feature Properties\n",
    "\n",
    "Now, let's analyze the properties of the extracted features. We'll load a sample of features from each model and examine their distributions, principal components, and similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features for analysis\n",
    "def load_features(feature_file, max_samples=100):\n",
    "    \"\"\"Load a sample of features from an H5 file.\"\"\"\n",
    "    # TODO: Implement feature loading from HDF5 file\n",
    "    # 1. Initialize an empty dictionary to store features\n",
    "    # 2. Open the HDF5 file\n",
    "    # 3. Get all image names from the file\n",
    "    # 4. Select a random sample if there are too many\n",
    "    # 5. Load features for the selected images\n",
    "    # 6. Return the dictionary of features\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Load a sample of features from each model for analysis\n",
    "sample_features = {}\n",
    "for name, config in results.items():\n",
    "    if os.path.exists(config['output_file']):\n",
    "        # Load sample features (max 50 for visualization purposes)\n",
    "        sample_features[name] = load_features(config['output_file'], max_samples=50)\n",
    "        print(f\"Loaded {len(sample_features[name])} sample features for {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Feature Distributions\n",
    "\n",
    "Let's examine the distribution of feature values for each model. This will give us insight into the statistical properties of the feature spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature distributions\n",
    "plt.figure(figsize=(15, 5 * len(sample_features)))\n",
    "\n",
    "for i, (name, features) in enumerate(sample_features.items()):\n",
    "    # Create subplot for this model\n",
    "    plt.subplot(len(sample_features), 1, i + 1)\n",
    "    \n",
    "    # Concatenate all features\n",
    "    all_values = np.concatenate([feat.flatten() for feat in features.values()])\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean = np.mean(all_values)\n",
    "    std = np.std(all_values)\n",
    "    min_val = np.min(all_values)\n",
    "    max_val = np.max(all_values)\n",
    "    \n",
    "    # Plot histogram with KDE\n",
    "    sns.histplot(all_values, bins=50, kde=True, alpha=0.7)\n",
    "    \n",
    "    # Add vertical line at mean\n",
    "    plt.axvline(mean, color='red', linestyle='--', label=f'Mean: {mean:.3f}')\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.xlabel('Feature Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{name} Feature Distribution - Mean: {mean:.3f}, Std: {std:.3f}, Range: [{min_val:.3f}, {max_val:.3f}]')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n{name} Feature Statistics:\")\n",
    "    print(f\"  Mean: {mean:.4f}\")\n",
    "    print(f\"  Std: {std:.4f}\")\n",
    "    print(f\"  Min: {min_val:.4f}\")\n",
    "    print(f\"  Max: {max_val:.4f}\")\n",
    "    print(f\"  25th percentile: {np.percentile(all_values, 25):.4f}\")\n",
    "    print(f\"  Median: {np.median(all_values):.4f}\")\n",
    "    print(f\"  75th percentile: {np.percentile(all_values, 75):.4f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Dimensionality Reduction with PCA\n",
    "\n",
    "Let's use Principal Component Analysis (PCA) to visualize the high-dimensional feature spaces in 2D. This will help us understand how images are organized in the feature space and if similar images are clustered together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature dimensionality reduction using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "plt.figure(figsize=(15, 5 * len(sample_features)))\n",
    "\n",
    "for i, (name, features) in enumerate(sample_features.items()):\n",
    "    # Flatten features and prepare data\n",
    "    X = np.array([feat for feat in features.values()])\n",
    "    image_names = list(features.keys())\n",
    "    \n",
    "    # Apply PCA to reduce to 2 dimensions\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    # Create subplot\n",
    "    plt.subplot(len(sample_features), 1, i + 1)\n",
    "    \n",
    "    # Plot PCA results with a scatter plot\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.8, s=50, \n",
    "                 c=np.arange(len(X_pca)), cmap='viridis')\n",
    "    \n",
    "    # Add hover annotations for interactive plots (if in notebook)\n",
    "    # This is a simple alternative when interactive plots aren't available\n",
    "    for j, img_name in enumerate(image_names):\n",
    "        # Only annotate a few points to avoid clutter\n",
    "        if j % 5 == 0:  # Annotate every 5th point\n",
    "            plt.annotate(img_name[:10], (X_pca[j, 0], X_pca[j, 1]), \n",
    "                         fontsize=8, alpha=0.7)\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title(f'{name} - PCA Visualization of Features', fontsize=14)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add colorbar to indicate point order\n",
    "    plt.colorbar(scatter, label='Image Index')\n",
    "    \n",
    "    # Print explained variance\n",
    "    print(f\"\\n{name} - PCA Explained Variance Ratio: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"Total variance explained: {np.sum(pca.explained_variance_ratio_):.4f} (or {np.sum(pca.explained_variance_ratio_) * 100:.2f}%)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Feature Similarity Analysis\n",
    "\n",
    "Let's analyze how similar different images are in the feature space. We'll compute cosine similarity between feature vectors and visualize the similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature similarities between images\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compare_feature_similarity(features, model_name, num_samples=10):\n",
    "    \"\"\"Compare feature similarity between random image pairs.\"\"\"\n",
    "    # TODO: Implement feature similarity analysis\n",
    "    # 1. Select random image samples\n",
    "    # 2. Create a feature matrix from the selected images\n",
    "    # 3. Compute cosine similarity matrix between features\n",
    "    # 4. Create and display a heatmap of the similarity matrix\n",
    "    # 5. Find and report the most similar and dissimilar image pairs\n",
    "    \n",
    "    return sim_matrix, sample_imgs\n",
    "\n",
    "# Compare feature similarity for each model\n",
    "for name, features in sample_features.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Comparing feature similarity for {name}:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    sim_matrix, sample_imgs = compare_feature_similarity(features, name, num_samples=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Visualize Sample Images with Feature Representations\n",
    "\n",
    "Let's visualize how the CNN models encode some sample images. This will help us understand what information is captured in the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images with their feature representations\n",
    "import random\n",
    "\n",
    "def show_image_with_features(image_name, features, model_name, images_dir):\n",
    "    \"\"\"Display an image alongside its feature representation.\"\"\"\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Image display (left subplot)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    img_path = os.path.join(images_dir, image_name)\n",
    "    if os.path.exists(img_path):\n",
    "        img = Image.open(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Image: {image_name}\", fontsize=12)\n",
    "        plt.axis('off')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, f\"Image not found\\n{img_path}\", \n",
    "                 ha='center', va='center', fontsize=12)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    # Feature visualization (right subplot)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    feat = features[image_name]\n",
    "    \n",
    "    # Plot feature values as a bar chart\n",
    "    # Limit to first 100 values if there are too many\n",
    "    max_display = min(100, len(feat))\n",
    "    x = np.arange(max_display)\n",
    "    plt.bar(x, feat[:max_display], width=0.8)\n",
    "    \n",
    "    # Add horizontal line at zero\n",
    "    plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Calculate and display statistics\n",
    "    mean = np.mean(feat)\n",
    "    std = np.std(feat)\n",
    "    \n",
    "    plt.title(f\"{model_name} Features\\nMean: {mean:.3f}, Std: {std:.3f}, Dimension: {len(feat)}\")\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Value')\n",
    "    \n",
    "    if max_display < len(feat):\n",
    "        plt.text(max_display/2, min(feat[:max_display]), f\"Showing first {max_display} of {len(feat)} features\", \n",
    "                 ha='center', fontsize=10)\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Select common images that appear in all feature sets\n",
    "common_images = set.intersection(*[set(features.keys()) for features in sample_features.values()])\n",
    "\n",
    "if common_images:\n",
    "    # Select a random image from common images\n",
    "    random_img = random.choice(list(common_images))\n",
    "    \n",
    "    print(f\"Displaying features for image: {random_img}\\n\")\n",
    "    \n",
    "    # Show the image with features from each model\n",
    "    for name, features in sample_features.items():\n",
    "        if random_img in features:\n",
    "            show_image_with_features(random_img, features, name, images_dir)\n",
    "else:\n",
    "    print(\"No common images found across all feature sets\")\n",
    "    \n",
    "    # Choose a random image from the first feature set\n",
    "    first_model = list(sample_features.keys())[0]\n",
    "    random_img = random.choice(list(sample_features[first_model].keys()))\n",
    "    \n",
    "    print(f\"Displaying features for image from {first_model}: {random_img}\\n\")\n",
    "    show_image_with_features(random_img, sample_features[first_model], first_model, images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusion\n",
    "\n",
    "Let's summarize what we've learned about the different feature extraction models and their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive comparison table\n",
    "model_comparison = []\n",
    "\n",
    "for name, config in cnn_models.items():\n",
    "    if name in results:\n",
    "        result = results[name]\n",
    "        output_file = result['output_file']\n",
    "        \n",
    "        # Calculate file size\n",
    "        file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "        storage_per_image_kb = file_size_mb * 1024 / result['num_features']\n",
    "        \n",
    "        # Get feature statistics if available\n",
    "        feature_mean = 'N/A'\n",
    "        feature_std = 'N/A'\n",
    "        if name in sample_features:\n",
    "            features = sample_features[name]\n",
    "            all_values = np.concatenate([feat.flatten() for feat in features.values()])\n",
    "            feature_mean = np.mean(all_values)\n",
    "            feature_std = np.std(all_values)\n",
    "        \n",
    "        model_comparison.append({\n",
    "            'Model': name,\n",
    "            'Parameters (M)': config['params'],\n",
    "            'Feature Dimension': result['feature_shape'],\n",
    "            'Storage per Image (KB)': storage_per_image_kb,\n",
    "            'Feature Mean': feature_mean,\n",
    "            'Feature Std': feature_std,\n",
    "            'Description': config['description']\n",
    "        })\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame(model_comparison).set_index('Model')\n",
    "\n",
    "# Format numeric columns\n",
    "if 'Storage per Image (KB)' in comparison_df.columns:\n",
    "    comparison_df['Storage per Image (KB)'] = comparison_df['Storage per Image (KB)'].map(lambda x: f\"{x:.2f}\" if isinstance(x, (int, float)) else x)\n",
    "if 'Feature Mean' in comparison_df.columns:\n",
    "    comparison_df['Feature Mean'] = comparison_df['Feature Mean'].map(lambda x: f\"{x:.4f}\" if isinstance(x, (int, float)) else x)\n",
    "if 'Feature Std' in comparison_df.columns:\n",
    "    comparison_df['Feature Std'] = comparison_df['Feature Std'].map(lambda x: f\"{x:.4f}\" if isinstance(x, (int, float)) else x)\n",
    "\n",
    "# Display comparison\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusion\n",
    "\n",
    "In this notebook, we've extracted image features using different pre-trained CNN architectures for our image captioning task. Let's summarize our findings:\n",
    "\n",
    "### Model Comparison\n",
    "\n",
    "1. **ResNet18**:\n",
    "   - Lightweight model with 11.7M parameters\n",
    "   - 512-dimensional features\n",
    "   - Good balance between speed and feature quality\n",
    "   - Smallest storage footprint\n",
    "\n",
    "2. **ResNet50**:\n",
    "   - Deeper model with 25.6M parameters\n",
    "   - 2048-dimensional features\n",
    "   - Higher quality features but slower processing\n",
    "   - Largest storage requirements\n",
    "\n",
    "3. **MobileNetV2**:\n",
    "   - Efficient model with only 3.5M parameters\n",
    "   - 1280-dimensional features\n",
    "   - Fastest processing time\n",
    "   - Designed for mobile and resource-constrained environments\n",
    "\n",
    "### Feature Analysis\n",
    "\n",
    "Our analysis revealed several insights about the extracted features:\n",
    "\n",
    "1. **Feature Distributions**: The feature values follow approximately normal distributions centered near zero, which is beneficial for subsequent neural network training.\n",
    "\n",
    "2. **Dimensionality Reduction**: PCA visualization showed that even in a reduced 2D space, similar images tend to cluster together, indicating that the features capture meaningful visual similarities.\n",
    "\n",
    "3. **Feature Similarity**: The cosine similarity analysis demonstrated that visually similar images have higher feature similarity scores, confirming that the feature representations preserve semantic relationships.\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "Based on our analysis, here's our recommendation for the image captioning task:\n",
    "\n",
    "- **For resource-constrained environments**: MobileNetV2 offers the best balance between performance and efficiency.\n",
    "- **For highest quality**: ResNet50 provides the most detailed feature representations.\n",
    "- **For balanced performance**: ResNet18 offers a good compromise between quality and efficiency.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "With our image features extracted and saved, we're now ready to move on to the next stages of our image captioning pipeline:\n",
    "\n",
    "1. Train an RNN/LSTM decoder that takes these image features as input and generates captions.\n",
    "2. Evaluate the captioning quality using different feature extractors to see which one performs best for our specific task.\n",
    "3. Fine-tune the entire system end-to-end if needed for further performance improvements.\n",
    "\n",
    "By pre-computing these features, we've significantly optimized our training workflow, allowing for faster experimentation with different language models without repeating the feature extraction step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
